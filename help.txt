
There is no book.csv file, as it is big so, please add in the folder before using running the code


Perfect â€” hereâ€™s your Task 1 full setup and execution guide (the â€œ1 master + 3 workersâ€ part) written exactly how you can copy, paste, and even include in your report.
Itâ€™s clean, simple, and doesnâ€™t record timing â€” just runs once to show output.
 
ğŸ§© 
Task 1 â€” Run T3 Master-Only with 1 Master + 3 Workers

Step 1 â€” Start Docker Cluster

Make sure Docker Desktop is running, then open your terminal and start all containers:
docker compose up -d
This launches:
â€¢	mpi_master (the coordinator)
â€¢	mpi_w1, mpi_w2, and mpi_w3 (the 3 workers)
 
Step 2 â€” Enter the Master Container
docker exec -it mpi_master bash
Now youâ€™re inside the master container as root.
 
Step 3 â€” Go to the Shared Workspace
cd /workspace
The /workspace folder is mounted from your host so all containers share the same files (code + dataset).
 
Step 4 â€” Run the T3 Master-Only Program
mpirun -n 4 -f /workspace/hostfile python /workspace/t3_master_only.py
Explanation:
â€¢	-n 4 â†’ total 4 processes (1 master + 3 workers)
â€¢	-f /workspace/hostfile â†’ tells MPI which containers to use
â€¢	The master distributes work; workers do the processing.
 
Expected Output Example
{
 'final_answer': 1192657,
 'chunkSizePerThread': [0, 1000000, 1000000, 1000000],
 'answerPerThread': [0, 395814, 397612, 399231],
 'totalTimeTaken': 17.78
}
âœ… This confirms your MPI cluster is working â€” the master coordinates while the 3 workers perform computation.
 
Thatâ€™s it for Task 1.
Once this runs successfully, youâ€™re ready to move on to Task 2 (analysis) using the timed loops for Q1â€“Q4.











Perfect â€” hereâ€™s everything in one clean, ready-to-paste version for your report and terminal 
 
ğŸ§® 
Part 2 â€“ Analysis (Q1â€“Q4 with 4â†’10 Workers)

Each section below creates a CSV file with execution times.
 
Q1 â€“ t3_master_only.py
cd /workspace
echo "containers,seconds" > q1_times.csv
for n in 4 5 6 7 8 9 10; do
  echo ">>> START n=$n $(date +%T)"
  start=$(date +%s)
  mpirun -n $n -f /workspace/hostfile python /workspace/t3_master_only.py > tmp_out.txt 2>&1
  end=$(date +%s)
  elapsed=$((end - start))
  echo ">>> DONE n=$n $(date +%T) (time=${elapsed}s)"
  cat tmp_out.txt
  echo "$n,$elapsed" | tee -a q1_times.csv
done
 
Q2 â€“ t3q2_master.py
cd /workspace
echo "containers,seconds" > q2_times.csv
for n in 4 5 6 7 8 9 10; do
  echo ">>> START n=$n $(date +%T)"
  start=$(date +%s)
  mpirun -n $n -f /workspace/hostfile python /workspace/t3q2_master.py > tmp_out.txt 2>&1
  end=$(date +%s)
  elapsed=$((end - start))
  echo ">>> DONE n=$n $(date +%T) (time=${elapsed}s)"
  cat tmp_out.txt
  echo "$n,$elapsed" | tee -a q2_times.csv
done
 
Q3 â€“ t3q3_master.py
cd /workspace
echo "containers,seconds" > q3_times.csv
for n in 4 5 6 7 8 9 10; do
  echo ">>> START n=$n $(date +%T)"
  start=$(date +%s)
  mpirun -n $n -f /workspace/hostfile python /workspace/t3q3_master.py > tmp_out.txt 2>&1
  end=$(date +%s)
  elapsed=$((end - start))
  echo ">>> DONE n=$n $(date +%T) (time=${elapsed}s)"
  cat tmp_out.txt
  echo "$n,$elapsed" | tee -a q3_times.csv
done
 
Q4 â€“ t3q4_master.py
cd /workspace
echo "containers,seconds" > q4_times.csv
for n in 4 5 6 7 8 9 10; do
  echo ">>> START n=$n $(date +%T)"
  start=$(date +%s)
  mpirun -n $n -f /workspace/hostfile python /workspace/t3q4_master.py > tmp_out.txt 2>&1
  end=$(date +%s)
  elapsed=$((end - start))
  echo ">>> DONE n=$n $(date +%T) (time=${elapsed}s)"
  cat tmp_out.txt
  echo "$n,$elapsed" | tee -a q4_times.csv
done
 
âš™ï¸ 
How to Run Everything
1.	Make sure Docker Desktop is running.
2.	Start the cluster: docker compose up -d
3.	Enter the master: docker exec -it mpi_master bash
4.	Paste and run the command block for each section (Q1â€“Q4).
 
ğŸ“Š 
Outputs Created
â€¢	q1_times.csv, q2_times.csv, q3_times.csv, q4_times.csv â†’ contain container count vs time
â€¢	You can use these for your MPI Execution Analysis graph.
 
âœ… Thatâ€™s everything you need for your report and demo â€” paste as-is into your terminal inside mpi_master.

